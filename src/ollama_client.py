"""OllamaåŸç”ŸAPIé€‚é…å™¨ - å°†OpenAI SDKè°ƒç”¨è½¬æ¢ä¸ºOllamaåŸç”Ÿ/api/generateæ¥å£"""

import httpx
import logging
import json
import time
import uuid
from typing import Any, Dict, AsyncGenerator, Optional, List
from openai.types.chat import ChatCompletion, ChatCompletionChunk
from openai.types.chat.chat_completion import Choice
from openai.types.chat.chat_completion_message import ChatCompletionMessage
from openai.types.completion_usage import CompletionUsage

logger = logging.getLogger(__name__)


class OllamaAsyncOpenAI:
    """
    OllamaåŸç”ŸAPIé€‚é…å™¨
    å°†OpenAI SDKæ ¼å¼çš„è¯·æ±‚è½¬æ¢ä¸ºOllamaåŸç”Ÿ/api/generateæ¥å£è°ƒç”¨
    å³ä½¿Ollamaåªéƒ¨ç½²äº†åŸç”ŸAPIï¼Œä¹Ÿèƒ½é€šè¿‡OpenAI SDKè®¿é—®
    """
    
    def __init__(self, base_url: str, api_key: str):
        # ç§»é™¤/v1åç¼€ï¼Œä½¿ç”¨åŸç”ŸAPIç«¯ç‚¹
        self.base_url = base_url.rstrip('/').replace('/v1', '')
        self.api_key = api_key
        
        # åˆ›å»ºHTTPå®¢æˆ·ç«¯
        self._http_client = httpx.AsyncClient(
            follow_redirects=False,
            timeout=httpx.Timeout(300.0)  # Ollamaç”Ÿæˆå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´
        )
        
        # åˆ›å»ºchatæ¥å£
        self.chat = ChatCompletions(self)
        
        logger.info(f"ğŸ”„ OllamaAsyncOpenAI (Native API Adapter) initialized")
        logger.info(f"ğŸ“ Target: {self.base_url}/api/generate")
    
    async def __aenter__(self):
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self._http_client.aclose()
    
    async def close(self):
        """å…³é—­HTTPå®¢æˆ·ç«¯"""
        await self._http_client.aclose()


class ChatCompletions:
    """Chat completionsæ¥å£é€‚é…å™¨"""
    
    def __init__(self, client: OllamaAsyncOpenAI):
        self.client = client
    
    @property
    def completions(self):
        """è¿”å›completionsæ¥å£"""
        return Completions(self.client)


class Completions:
    """Completionsæ¥å£å®ç° - OpenAIåˆ°OllamaåŸç”ŸAPIçš„æ ¸å¿ƒé€‚é…å™¨"""
    
    def __init__(self, client: OllamaAsyncOpenAI):
        self.client = client
    
    async def create(self, **kwargs) -> Any:
        """
        åˆ›å»ºchat completionè¯·æ±‚ - è½¬æ¢ä¸ºOllamaåŸç”Ÿ/api/generateè°ƒç”¨
        """
        logger.info(f"ğŸ”„ === OpenAI -> Ollama Native API Conversion ===")
        logger.info(f"ğŸ“ Target: {self.client.base_url}/api/generate")
        logger.info(f"ğŸ¤– Model: {kwargs.get('model', 'Unknown')}")
        logger.info(f"ğŸ“¡ Stream: {kwargs.get('stream', False)}")
        
        # è½¬æ¢OpenAIæ ¼å¼åˆ°OllamaåŸç”Ÿæ ¼å¼
        ollama_request = self._convert_openai_to_ollama(kwargs)
        
        # æ„å»ºè¯·æ±‚URL - ä½¿ç”¨OllamaåŸç”ŸAPIç«¯ç‚¹
        url = f"{self.client.base_url}/api/generate"
        
        # è®¾ç½®è¯·æ±‚å¤´
        headers = {
            'Content-Type': 'application/json',
        }
        
        try:
            logger.info(f"ğŸ“¤ Sending request to: {url}")
            logger.info(f"ğŸ“‹ Ollama request: {json.dumps(ollama_request, indent=2, ensure_ascii=False)}")
            
            # å‘é€è¯·æ±‚åˆ°OllamaåŸç”ŸAPI
            response = await self.client._http_client.post(
                url,
                headers=headers,
                json=ollama_request
            )
            
            response.raise_for_status()
            logger.info(f"âœ… Ollama native API call successful - Status: {response.status_code}")
            
            # å¤„ç†å“åº”
            if ollama_request.get('stream', False):
                return await self._handle_stream_response(response, kwargs)
            else:
                # éæµå¼å“åº”
                result = response.json()
                return self._convert_ollama_to_openai(result, kwargs)
                
        except httpx.HTTPStatusError as e:
            logger.error(f"âŒ Ollama Native API Error: {e.response.status_code}")
            logger.error(f"ğŸ“ Request URL: {e.request.url}")
            logger.error(f"ğŸ“„ Response text: {e.response.text}")
            
            if e.response.status_code == 400:
                logger.error(f"ğŸ” 400 Bad Request Analysis:")
                logger.error(f"  - Check if model '{kwargs.get('model')}' is available: ollama list")
                logger.error(f"  - Verify Ollama service is running: ollama serve")
                logger.error(f"  - Check if /api/generate endpoint is accessible")
                
                # å°è¯•è§£æé”™è¯¯å“åº”
                try:
                    error_data = e.response.json()
                    logger.error(f"  - Ollama error details: {error_data}")
                except:
                    pass
            
            raise
        except Exception as e:
            logger.error(f"âŒ Ollama native API call failed: {str(e)}")
            logger.error(f"ğŸ” Exception type: {type(e).__name__}")
            raise
    
    def _convert_openai_to_ollama(self, openai_params: dict) -> dict:
        """å°†OpenAIæ ¼å¼çš„è¯·æ±‚è½¬æ¢ä¸ºOllamaåŸç”ŸAPIæ ¼å¼"""
        logger.info(f"ğŸ”„ Converting OpenAI format to Ollama native format")
        
        # æå–åŸºæœ¬å‚æ•°
        model = openai_params.get('model')
        messages = openai_params.get('messages', [])
        stream = openai_params.get('stream', False)
        
        # éªŒè¯å¿…éœ€å‚æ•°
        if not model:
            raise ValueError("Model is required")
        if not messages:
            raise ValueError("Messages are required")
        
        # å°†messagesè½¬æ¢ä¸ºå•ä¸ªprompt
        prompt = self._messages_to_prompt(messages)
        
        # æ„å»ºOllamaåŸç”ŸAPIè¯·æ±‚
        ollama_request = {
            "model": model,
            "prompt": prompt,
            "stream": stream
        }
        
        # æ·»åŠ å¯é€‰å‚æ•°
        if 'temperature' in openai_params:
            ollama_request['options'] = ollama_request.get('options', {})
            ollama_request['options']['temperature'] = openai_params['temperature']
        
        if 'max_tokens' in openai_params:
            ollama_request['options'] = ollama_request.get('options', {})
            ollama_request['options']['num_predict'] = openai_params['max_tokens']
        
        # æ·»åŠ å…¶ä»–Ollamaæ”¯æŒçš„å‚æ•°
        if 'top_p' in openai_params:
            ollama_request['options'] = ollama_request.get('options', {})
            ollama_request['options']['top_p'] = openai_params['top_p']
        
        logger.info(f"âœ… Converted to Ollama format: model={model}, prompt_length={len(prompt)}")
        return ollama_request
    
    def _messages_to_prompt(self, messages: List[Dict]) -> str:
        """å°†OpenAI messagesæ ¼å¼è½¬æ¢ä¸ºå•ä¸ªpromptå­—ç¬¦ä¸²"""
        prompt_parts = []
        
        for message in messages:
            role = message.get('role', '')
            content = message.get('content', '')
            
            if role == 'system':
                prompt_parts.append(f"System: {content}")
            elif role == 'user':
                prompt_parts.append(f"User: {content}")
            elif role == 'assistant':
                prompt_parts.append(f"Assistant: {content}")
            else:
                # æœªçŸ¥è§’è‰²ï¼Œç›´æ¥æ·»åŠ å†…å®¹
                prompt_parts.append(content)
        
        # æ·»åŠ æœ€åçš„Assistantæç¤º
        prompt_parts.append("Assistant:")
        
        prompt = "\n\n".join(prompt_parts)
        logger.info(f"ğŸ“ Converted {len(messages)} messages to prompt ({len(prompt)} chars)")
        return prompt
    
    def _convert_ollama_to_openai(self, ollama_response: dict, original_params: dict) -> ChatCompletion:
        """å°†OllamaåŸç”ŸAPIå“åº”è½¬æ¢ä¸ºOpenAIæ ¼å¼"""
        logger.info(f"ğŸ”„ Converting Ollama response to OpenAI format")
        
        # æå–å“åº”å†…å®¹
        response_text = ollama_response.get('response', '')
        model = ollama_response.get('model', original_params.get('model', 'unknown'))
        
        # åˆ›å»ºOpenAIæ ¼å¼çš„å“åº”
        completion = ChatCompletion(
            id=f"chatcmpl-{uuid.uuid4().hex[:8]}",
            object="chat.completion",
            created=int(time.time()),
            model=model,
            choices=[
                Choice(
                    index=0,
                    message=ChatCompletionMessage(
                        role="assistant",
                        content=response_text
                    ),
                    finish_reason="stop"
                )
            ],
            usage=self._create_usage_stats(ollama_response)
        )
        
        logger.info(f"âœ… Converted to OpenAI format: {len(response_text)} chars")
        return completion
    
    def _create_usage_stats(self, ollama_response: dict) -> CompletionUsage:
        """ä»Ollamaå“åº”åˆ›å»ºä½¿ç”¨ç»Ÿè®¡"""
        # OllamaåŸç”ŸAPIå¯èƒ½åŒ…å«tokenç»Ÿè®¡ä¿¡æ¯
        prompt_tokens = ollama_response.get('prompt_eval_count', 0)
        completion_tokens = ollama_response.get('eval_count', 0)
        total_tokens = prompt_tokens + completion_tokens
        
        return CompletionUsage(
            prompt_tokens=prompt_tokens,
            completion_tokens=completion_tokens,
            total_tokens=total_tokens
        )
    
    async def _handle_stream_response(self, response: httpx.Response, original_params: dict) -> AsyncGenerator[ChatCompletionChunk, None]:
        """å¤„ç†Ollamaæµå¼å“åº”å¹¶è½¬æ¢ä¸ºOpenAIæ ¼å¼"""
        logger.info(f"ğŸŒŠ Handling Ollama stream response")
        
        async for line in response.aiter_lines():
            if line.strip():
                try:
                    # è§£æOllamaæµå¼å“åº”
                    ollama_chunk = json.loads(line)
                    
                    # è½¬æ¢ä¸ºOpenAIæ ¼å¼çš„chunk
                    openai_chunk = self._convert_ollama_chunk_to_openai(ollama_chunk, original_params)
                    if openai_chunk:
                        yield openai_chunk
                        
                except json.JSONDecodeError:
                    logger.warning(f"Failed to parse stream line: {line}")
                    continue
    
    def _convert_ollama_chunk_to_openai(self, ollama_chunk: dict, original_params: dict) -> Optional[ChatCompletionChunk]:
        """å°†Ollamaæµå¼chunkè½¬æ¢ä¸ºOpenAIæ ¼å¼"""
        response_text = ollama_chunk.get('response', '')
        done = ollama_chunk.get('done', False)
        
        if response_text or done:
            from openai.types.chat.chat_completion_chunk import ChoiceDelta
            from openai.types.chat.chat_completion_chunk import Choice as ChunkChoice
            
            chunk = ChatCompletionChunk(
                id=f"chatcmpl-{uuid.uuid4().hex[:8]}",
                object="chat.completion.chunk",
                created=int(time.time()),
                model=ollama_chunk.get('model', original_params.get('model', 'unknown')),
                choices=[
                    ChunkChoice(
                        index=0,
                        delta=ChoiceDelta(
                            content=response_text if not done else None
                        ),
                        finish_reason="stop" if done else None
                    )
                ]
            )
            return chunk
        
        return None


# OllamaåŸç”ŸAPIé€‚é…å™¨å®ç°å®Œæˆ
# æ”¯æŒå°†OpenAI SDKè°ƒç”¨è½¬æ¢ä¸ºOllamaåŸç”Ÿ/api/generateæ¥å£
# å³ä½¿Ollamaåªéƒ¨ç½²äº†åŸç”ŸAPIï¼Œä¹Ÿèƒ½é€šè¿‡OpenAI SDKè®¿é—®
